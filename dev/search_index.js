var documenterSearchIndex = {"docs":
[{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"CurrentModule = SemanticCaches","category":"page"},{"location":"api_reference/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"API reference  for SemanticCaches.","category":"page"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"Modules = [SemanticCaches]","category":"page"},{"location":"api_reference/#SemanticCaches.HashCache","page":"API Reference","title":"SemanticCaches.HashCache","text":"HashCache\n\nA cache that uses string hashes to find the exactly matching items. Useful for long input strings, which cannot be embedded quickly.\n\nAny incoming request must match key exactly (in lookup), otherwise it's not accepted. key represents what user finds meaningful to be strictly matching (eg, model name, temperature, etc).\n\nFields\n\nitems: A vector of cached items (type CachedItem)\nlookup: A dictionary that maps keys to the indices of the items that have that key.\nitems_lock: A lock for the items vector.\nlookup_lock: A lock for the lookup dictionary.\n\n\n\n\n\n","category":"type"},{"location":"api_reference/#SemanticCaches.HashCache-Tuple{String, String}","page":"API Reference","title":"SemanticCaches.HashCache","text":"(cache::HashCache)(key::String, fuzzy_input::String; verbose::Integer = 0, min_similarity::Real = 1.0)\n\nFinds the item that EXACTLY matches the provided cache key and EXACTLY matches the hash of fuzzy_input.\n\nArguments\n\nkey::String: The key to match exactly.\nfuzzy_input::String: The input to compare the hash of.\nverbose::Integer = 0: The verbosity level.\nmin_similarity::Real = 1.0: The minimum similarity (we expect exact match defined as 1.0).\n\nReturns\n\nA CachedItem:\n\nIf an exact match is found, the output field is set to the cached output.\nIf no exact match is found, the output field is set to nothing.\n\nYou can validate if an item has been found by checking if output is not nothing or simply isvalid(item).\n\nExample\n\ncache = HashCache()\nitem = cache(\"key1\", \"fuzzy_input\")\n\n## add it to cache if new\nif !isvalid(item)\n    # calculate the expensive output\n    output = expensive_calculation()\n    item.output = output\n    ## add it to cache\n    push!(cache, item)\nend\n\n# If you ask again, it will be faster because it's in the cache\nitem = cache(\"key1\", \"fuzzy_input\")\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#SemanticCaches.SemanticCache","page":"API Reference","title":"SemanticCaches.SemanticCache","text":"SemanticCache\n\nA cache that stores embeddings and uses semantic search to find the most relevant items.\n\nAny incoming request must match key exactly (in lookup), otherwise it's not accepted. key represents what user finds meaningful to be strictly matching (eg, model name, temperature, etc).\n\nFields\n\nitems: A vector of cached items (type CachedItem)\nlookup: A dictionary that maps keys to the indices of the items that have that key.\nitems_lock: A lock for the items vector.\nlookup_lock: A lock for the lookup dictionary.\n\n\n\n\n\n","category":"type"},{"location":"api_reference/#SemanticCaches.SemanticCache-Tuple{String, String}","page":"API Reference","title":"SemanticCaches.SemanticCache","text":"(cache::SemanticCache)(\n    key::String, fuzzy_input::String; verbose::Integer = 0, min_similarity::Real = 0.95)\n\nFinds the item that EXACTLY matches the provided cache key and is the most similar given its embedding. Similarity must be at least min_similarity.  Search is done via cosine similarity (dot product).\n\nArguments\n\nkey::String: The key to match exactly.\nfuzzy_input::String: The input to embed and compare to the cache.\nverbose::Integer = 0: The verbosity level.\nmin_similarity::Real = 0.95: The minimum similarity.\n\nReturns\n\nA CachedItem:\n\nIf the similarity is above min_similarity, the output field is set to the cached output.\nIf the similarity is below min_similarity, the output field is set to nothing.\n\nYou can validate if an item has been found by checking if output is not nothing or simply isvalid(item).\n\nExample\n\ncache = SemanticCache()\nitem = cache(\"key1\", \"fuzzy_input\"; min_similarity=0.95)\n\n## add it to cache if new\nif !isvalid(item)\n    # calculate the expensive output\n    output = expensive_calculation()\n    item.output = output\n    ## add it to cache\n    push!(cache, item)\nend\n\n# If you ask again, it will be faster because it's in the cache\nitem = cache(\"key1\", \"fuzzy_input\"; min_similarity=0.95)\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#SemanticCaches.similarity-Tuple{HashCache, Vector{CachedItem}, Vector{Int64}, UInt64}","page":"API Reference","title":"SemanticCaches.similarity","text":"similarity(cache::HashCache, items::Vector{CachedItem},\n    indices::Vector{Int}, hash::UInt64)\n\nFinds the items with the exact hash as hash.\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#SemanticCaches.similarity-Tuple{SemanticCache, Vector{CachedItem}, Vector{Int64}, Vector{Float32}}","page":"API Reference","title":"SemanticCaches.similarity","text":"similarity(cache::SemanticCache, items::Vector{CachedItem},\n    indices::Vector{Int}, embedding::Vector{Float32})\n\nFinds the most similar item in the cache to the given embedding. Search is done via cosine similarity (dot product).\n\nArguments\n\ncache::SemanticCache: The cache to search in.\nitems::Vector{CachedItem}: The items to search in.\nindices::Vector{Int}: The indices of the items to search in.\nembedding::Vector{Float32}: The embedding to search for.\n\nReturns\n\nA tuple (max_sim, max_idx) where\n\nmax_sim: The maximum similarity.\nmax_idx: The index of the most similar item.\n\nNotes\n\nThe return item is not guaranteed to be very similar, you need to check if the similarity is high enough.\nWe assume that embeddings are normalized to have L2 norm 1, so Cosine similarity is the same as dot product.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SemanticCaches","category":"page"},{"location":"#SemanticCaches","page":"Home","title":"SemanticCaches","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for SemanticCaches.","category":"page"},{"location":"","page":"Home","title":"Home","text":"SemanticCaches.jl is a very hacky implementation of a semantic cache for AI applications to save time and money with repeated requests. It's not particularly fast, because we're trying to prevent API calls that can take even 20 seconds.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that we're using a tiny BERT model with a maximum chunk size of 512 tokens to provide fast local embeddings running on a CPU. For longer sentences, we split them in several chunks and consider their average embedding, but use it carefully! The latency can sky rocket and become worse than simply calling the original API.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install SemanticCaches.jl, simply add this repository (package is not yet registered).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/svilupp/SemanticCaches.jl\")","category":"page"},{"location":"#Quick-Start-Guide","page":"Home","title":"Quick Start Guide","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\nusing SemanticCaches\n\nsem_cache = SemanticCache()\n# First argument: the key must always match exactly, eg, model, temperature, etc\n# Second argument: the input text to be compared with the cache, can be fuzzy matched\nitem = sem_cache(\"key1\", \"say hi!\"; verbose = 1) # notice the verbose flag it can 0,1,2 for different level of detail\nif !isvalid(item)\n    @info \"cache miss!\"\n    item.output = \"expensive result X\"\n    # Save the result to the cache for future reference\n    push!(sem_cache, item)\nend\n\n# If practice, long texts may take too long to embed even with our tiny model\n# so let's not compare anything above 2000 tokens =~ 5000 characters (threshold of c. 100ms)\n\nhash_cache = HashCache()\ninput = \"say hi\"\ninput = \"say hi \"^1000\n\nactive_cache = length(input) > 5000 ? hash_cache : sem_cache\nitem = active_cache(\"key1\", input; verbose = 1)\n\nif !isvalid(item)\n    @info \"cache miss!\"\n    item.output = \"expensive result X\"\n    push!(active_cache, item)\nend","category":"page"},{"location":"#How-it-Works","page":"Home","title":"How it Works","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The primary objective of building this package was to cache expensive API calls to GenAI models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The system offers exact matching (faster, HashCache) and semantic similarity lookup (slower, SemanticCache) of STRING inputs. In addition, all requests are first compared on a “cache key”, which presents a key that must always match exactly for requests to be considered interchangeable (eg, same model, same provider, same temperature, etc).  You need to choose the appropriate cache key and input depending on your use case. This default choice for the cache key should be the model name.","category":"page"},{"location":"","page":"Home","title":"Home","text":"What happens when you call the cache (provide cache_key and string_input)?","category":"page"},{"location":"","page":"Home","title":"Home","text":"All cached outputs are stored in a vector cache.items.\nWhen we receive a request, the cache_key is looked up to find indices of the corresponding items in items. If cache_key is not found, we return CachedItem with an empty output field (ie, isvalid(item) == false).\nWe embed the string_input using a tiny BERT model and normalize the embeddings (to make it easier to compare the cosine distance later).\nWe then compare the cosine distance with the embeddings of the cached items.\nIf the cosine distance is higher than min_similarity threshold, we return the cached item (The output can be found in the field item.output).","category":"page"},{"location":"","page":"Home","title":"Home","text":"If we haven't found any cached item, we return CachedItem with an empty output field (ie, isvalid(item) == false). Once you calculate the response and save it in item.output, you can push the item to the cache by calling push!(cache, item).","category":"page"},{"location":"#Suitable-Use-Cases","page":"Home","title":"Suitable Use Cases","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is great if you know you will have a smaller volume of requests (eg, <10k per session or machine).\nIt’s ideal to reduce the costs of running your evals, because even when you change your RAG pipeline configuration many of the calls will be repeated and can take advantage of caching.\nLastly, this package can be really useful for demos and small user applications, where you can know some of the system inputs upfront, so you can cache them and show incredible response times!\nThis package is NOT suitable for production systems with hundreds of thousands of requests and remember that this is a very basic cache that you need to manually invalidate over time!","category":"page"},{"location":"#Advanced-Usage","page":"Home","title":"Advanced Usage","text":"","category":"section"},{"location":"#Caching-HTTP-Requests","page":"Home","title":"Caching HTTP Requests","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Based on your knowledge of the API calls made, you need determine the: 1) cache key (separate store of cached items, eg, different models or temperatures) and 2) how to unpack the HTTP request into a string (eg, unwrap and join the formatted message contents for OpenAI API).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here's a brief outline of how you can use SemanticCaches.jl with PromptingTools.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using PromptingTools\nusing SemanticCaches\nusing HTTP\n\n## Define the new caching mechanism as a layer for HTTP\n## See documentation [here](https://juliaweb.github.io/HTTP.jl/stable/client/#Quick-Examples)\nmodule MyCache\n\nusing HTTP, JSON3\nusing SemanticCaches\n\nconst SEM_CACHE = SemanticCache()\nconst HASH_CACHE = HashCache()\n\nfunction cache_layer(handler)\n    return function (req; cache_key::Union{AbstractString,Nothing}=nothing, kw...)\n        # only apply the cache layer if the user passed `cache_key`\n        # we could also use the contents of the payload, eg, `cache_key = get(body, \"model\", \"unknown\")`\n        if req.method == \"POST\" && cache_key !== nothing\n            body = JSON3.read(copy(req.body))\n            if occursin(\"v1/chat/completions\", req.target)\n                ## We're in chat completion endpoint\n                input = join([m[\"content\"] for m in body[\"messages\"]], \" \")\n            elseif occursin(\"v1/embeddings\", req.target)\n                ## We're in embedding endpoint\n                input = body[\"input\"]\n            else\n                ## Skip, unknown API\n                return handler(req; kw...)\n            end\n            ## Check the cache\n            @info \"Check if we can cache this request ($(length(input)) chars)\"\n            active_cache = length(input) > 5000 ? HASH_CACHE : SEM_CACHE\n            item = active_cache(\"key1\", input; verbose=2) # change verbosity to 0 to disable detailed logs\n            if !isvalid(item)\n                @info \"Cache miss! Pinging the API\"\n                # pass the request along to the next layer by calling `cache_layer` arg `handler`\n                resp = handler(req; kw...)\n                item.output = resp\n                # Let's remember it for the next time\n                push!(active_cache, item)\n            end\n            ## Return the calculated or cached result\n            return item.output\n        end\n        # pass the request along to the next layer by calling `cache_layer` arg `handler`\n        # also pass along the trailing keyword args `kw...`\n        return handler(req; kw...)\n    end\nend\n\n# Create a new client with the auth layer added\nHTTP.@client [cache_layer]\n\nend # module\n\n\n# Let's push the layer globally in all HTTP.jl requests\nHTTP.pushlayer!(MyCache.cache_layer)\n# HTTP.poplayer!() # to remove it later\n\n# Let's call the API\n@time msg = aigenerate(\"What is the meaning of life?\"; http_kwargs=(; cache_key=\"key1\"))\n\n# The first call will be slow as usual, but any subsequent call should be pretty quick - try it a few times!","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also use it for embeddings, eg, ","category":"page"},{"location":"","page":"Home","title":"Home","text":"@time msg = aiembed(\"how is it going?\"; http_kwargs=(; cache_key=\"key2\")) # 0.7s\n@time msg = aiembed(\"how is it going?\"; http_kwargs=(; cache_key=\"key2\")) # 0.02s\n\n# Even with a tiny difference (no question mark), it still picks the right cache\n@time msg = aiembed(\"how is it going\"; http_kwargs=(; cache_key=\"key2\")) # 0.02s","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can remove the cache layer by calling HTTP.poplayer!() (and add it again if you made some changes).","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can probe the cache by calling MyCache.SEM_CACHE (eg, MyCache.SEM_CACHE.items[1]).","category":"page"},{"location":"#Frequently-Asked-Questions","page":"Home","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"How is the performance?","category":"page"},{"location":"","page":"Home","title":"Home","text":"The majority of time will be spent in 1) tiny embeddings (for large texts, eg, thousands of tokens) and in calculating cosine similarity (for large caches, eg, over 10k items).","category":"page"},{"location":"","page":"Home","title":"Home","text":"For reference, embedding smaller texts like questions to embed takes only a few milliseconds. Embedding 2000 tokens can take anywhere from 50-100ms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When it comes to the caching system, there are many locks to avoid faults, but the overhead is still negligible - I ran experiments with 100k sequential insertions and the time per item was only a few milliseconds (dominated by the cosine similarity). If your bottleneck is in the cosine similarity calculation (c. 4ms for 100k items), consider moving vectors into a matrix for continuous memory and/or use Boolean embeddings with Hamming distance (XOR operator, c. order of magnitude speed up).","category":"page"},{"location":"","page":"Home","title":"Home","text":"All in all, the system is faster than necessary for normal workloads with thousands of cached items. You’re more likely to have GC and memory problems if your payloads are big (consider swapping to disk) than to face compute bounds. Remember that the motivation is to prevent API calls that take anywhere between 1-20 seconds!","category":"page"},{"location":"","page":"Home","title":"Home","text":"How to measure the time it takes to do X?","category":"page"},{"location":"","page":"Home","title":"Home","text":"Have a look at the example snippets below - time whichever part of it you’re interested in.","category":"page"},{"location":"","page":"Home","title":"Home","text":"\nsem_cache = SemanticCache()\n# First argument: the key must always match exactly, eg, model, temperature, etc\n# Second argument: the input text to be compared with the cache, can be fuzzy matched\nitem = sem_cache(\"key1\", \"say hi!\"; verbose = 1) # notice the verbose flag it can 0,1,2 for different level of detail\nif !isvalid(item)\n    @info \"cache miss!\"\n    item.output = \"expensive result X\"\n    # Save the result to the cache for future reference\n    push!(sem_cache, item)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Embedding only (to tune the min_similarity threshold or to time the embedding)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SemanticCaches.FlashRank: embed\nusing SemanticCaches: EMBEDDER\n\n@time res = embed(EMBEDDER, \"say hi\")\n#   0.000903 seconds (104 allocations: 19.273 KiB)\n# see res.elapsed or res.embeddings\n\n# long inputs (split into several chunks and then combining the embeddings)\n@time embed(EMBEDDER, \"say hi \"^1000)\n#   0.032148 seconds (8.11 k allocations: 662.656 KiB)","category":"page"},{"location":"","page":"Home","title":"Home","text":"How to set the min_similarity threshold?","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can set the min_similarity threshold by adding the kwarg active_cache(\"key1\", input; verbose=2, min_similarity=0.95).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The default is 0.95, which is a very high threshold. For practical purposes, I'd recommend ~0.9. If you're expecting some typos, you can go even a bit lower (eg, 0.85). Be careful though - it's hard to embed super short sequences well! You might want to adjust the threshold depending on the length of the input.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to calculate the cosine similarity, remember to normalize the embeddings first or divide the dot product by the norms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SemanticCaches.LinearAlgebra: normalize, norm, dot\ncosine_similarity = dot(r1.embeddings, r2.embeddings) / (norm(r1.embeddings) * norm(r2.embeddings))\n# remember that 1 is the best similarity, -1 is the exact opposite","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can compare different inputs to determine the best threshold for your use cases","category":"page"},{"location":"","page":"Home","title":"Home","text":"emb1 = embed(EMBEDDER, \"How is it going?\") |> x -> vec(x.embeddings) |> normalize\nemb2 = embed(EMBEDDER, \"How is it goin'?\") |> x -> vec(x.embeddings) |> normalize\ndot(emb1, emb2) # 0.944\n\nemb1 = embed(EMBEDDER, \"How is it going?\") |> x -> vec(x.embeddings) |> normalize\nemb2 = embed(EMBEDDER, \"How is it goin'\") |> x -> vec(x.embeddings) |> normalize\ndot(emb1, emb2) # 0.920","category":"page"},{"location":"","page":"Home","title":"Home","text":"How to debug it?","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enable verbose logging by adding the kwarg verbose = 2, eg, item = active_cache(\"key1\", input; verbose=2).","category":"page"}]
}
